{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6262219f-7b48-4a41-985d-b9fe61d90280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa667aeb-cf8b-4b1f-82a9-f3e2e312c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processes a single wide-format MISO Day-Ahead EPNode CSV and returns a long-format DataFrame with datetime, node, and LMP components.\n",
    "#Data Source: https://www.misoenergy.org/markets-and-operations/real-time--market-data/market-reports/#nt=%2FMarketReportType%3AHistorical%20LMP%2FMarketReportName%3ADay-Ahead%20EPNode%20LMP%20(zip)&t=10&p=3&s=MarketReportPublished&sd=desc\n",
    "def process_wide_format_safely(file_path):\n",
    "\n",
    "    # Step 1: Read lines and find header row\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Locate header row (should contain \"EPNode\" and \"HE1\")\n",
    "    header_row_index = next(\n",
    "        (i for i, line in enumerate(lines) if \"EPNode\" in line and \"HE1\" in line),\n",
    "        None\n",
    "    )\n",
    "\n",
    "    if header_row_index is None:\n",
    "        raise ValueError(\"Could not find valid header row in file.\")\n",
    "\n",
    "    # Step 2: Extract market date (e.g., \"Market Day: 06/05/2025\")\n",
    "    market_day_line = next(\n",
    "        (line for line in lines if \"Market Day\" in line),\n",
    "        None\n",
    "    )\n",
    "    if not market_day_line:\n",
    "        raise ValueError(\"Market Day not found in file.\")\n",
    "\n",
    "    market_day_str = market_day_line.split(\":\")[-1].strip()\n",
    "    market_date = datetime.strptime(market_day_str, \"%m/%d/%Y\")\n",
    "\n",
    "    # Step 3: Load CSV from the correct header row\n",
    "    df = pd.read_csv(file_path, skiprows=header_row_index)\n",
    "\n",
    "    # Step 4: Convert wide to long format\n",
    "    long_data = []\n",
    "\n",
    "    for i in range(0, len(df) - 2, 3):  # Process 3 rows at a time (LMP, MCC, MLC)\n",
    "        try:\n",
    "            row_lmp = df.iloc[i]\n",
    "            row_mcc = df.iloc[i + 1]\n",
    "            row_mlc = df.iloc[i + 2]\n",
    "\n",
    "            # Safe access to first column (node name), drop leading \"L\"\n",
    "            node = \" \".join(str(row_lmp.iloc[0]).split()[1:])\n",
    "\n",
    "            for hour in range(1, 25):  # HE1 to HE24\n",
    "                hour_col = f\"HE{hour}\"\n",
    "                dt = market_date + timedelta(hours=hour - 1)\n",
    "\n",
    "                long_data.append({\n",
    "                    \"datetime\": dt,\n",
    "                    \"node\": node,\n",
    "                    \"lmp_total\": float(row_lmp[hour_col]),\n",
    "                    \"congestion\": float(row_mcc[hour_col]),\n",
    "                    \"loss\": float(row_mlc[hour_col])\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing rows {i}-{i+2}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(long_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "610dce93-3ec9-45ab-98d9-5959de3fd8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file data/cleaned/DA/miso_da_combined_clean.csv already exists. Skipping processing.\n"
     ]
    }
   ],
   "source": [
    "dirty_folder = \"data/dirty/DA/Wide\"\n",
    "cleaned_folder = \"data/dirty/DA/Long\"\n",
    "\n",
    "os.makedirs(cleaned_folder, exist_ok=True)  # make sure cleaned folder exists\n",
    "file_path = 'data/cleaned/DA/miso_da_combined_clean.csv'\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"The file {file_path} does not exist. Running the processing code...\")\n",
    "    for filename in sorted(os.listdir(dirty_folder)):\n",
    "        if filename.startswith(\"DA_Load_EPNodes\") and filename.endswith(\".csv\"):\n",
    "            input_path = os.path.join(dirty_folder, filename)\n",
    "            output_path = os.path.join(cleaned_folder, filename.replace(\"DA_Load_EPNodes\", \"miso_da\")[:-4] + \"_clean.csv\")\n",
    "            \n",
    "            print(f\"Processing {filename} ...\")\n",
    "            try:\n",
    "                df = process_wide_format_safely(input_path)\n",
    "                df.to_csv(output_path, index=False)\n",
    "                print(f\"Saved cleaned file to {output_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {filename}: {e}\")\n",
    "else:\n",
    "    print(f\"The file {file_path} already exists. Skipping processing.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faffd9de-f8ea-4929-b6f2-a8a806e44ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_cleaned_csvs(folder=\"data/dirty/DA/Long\", prefix=\"miso_da_\", ext=\".csv\"):\n",
    "    all_dfs = []\n",
    "    \n",
    "    for filename in sorted(os.listdir(folder)):\n",
    "        if filename.startswith(prefix) and filename.endswith(ext):\n",
    "            file_path = os.path.join(folder, filename)\n",
    "            print(f\"üìÑ Loading: {filename}\")\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, parse_dates=[\"datetime\"])\n",
    "                all_dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to load {filename}: {e}\")\n",
    "\n",
    "    if not all_dfs:\n",
    "        print(\"‚ùå No files combined.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    combined_df.sort_values(by=[\"datetime\", \"node\"], inplace=True)\n",
    "    combined_df.to_csv(\"data/cleaned/DA/miso_da_combined_clean.csv\")\n",
    "    print(f\"‚úÖ Combined dataframe saved\")\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4f54b6c-f90b-45b0-a5e0-01eb55043da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file data/cleaned/DA/miso_da_combined_clean.csv already exists. Skipping processing.\n"
     ]
    }
   ],
   "source": [
    "file_path = 'data/cleaned/DA/miso_da_combined_clean.csv'\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"The file {file_path} does not exist. Running the processing code...\")\n",
    "    combine_cleaned_csvs()\n",
    "    # Make sure the directory exists before saving\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    print(f\"Processing complete. File saved to {file_path}\")\n",
    "else:\n",
    "    print(f\"The file {file_path} already exists. Skipping processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b73d3ee2-ccf3-4350-bfd5-8a71acd7afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_prices = pd.read_csv('data/cleaned/MPMA clearing/20250501_ftr_mpma_results/SourceSinkShadowPrices_May25_AUCTION_May25Auc_Round_1.csv')\n",
    "DA_LMP = pd.read_csv('data/cleaned/DA/miso_da_combined_clean.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c1debd0-c1de-4e70-b142-db49662b661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LMP nodes (your nodes to match)\n",
    "lmp_nodes = list(set(DA_LMP['node']))\n",
    "\n",
    "# SourceSink nodes from FTR auction paths\n",
    "source_sink_nodes = list(set(shadow_prices['SourceSink']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fab7e93-4277-495c-8ebe-ac90a711370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store results\n",
    "results = []\n",
    "\n",
    "for lmp in lmp_nodes:\n",
    "    extraction_result = process.extractOne(lmp, source_sink_nodes, scorer=fuzz.token_sort_ratio)\n",
    "    match = extraction_result[0]  # First element is the matched string\n",
    "    score = extraction_result[1]  # Second element is the score\n",
    "    \n",
    "    results.append({\n",
    "        'lmp_node': lmp,\n",
    "        'matched_source_sink': match,\n",
    "        'match_score': score\n",
    "    })\n",
    "\n",
    "matched_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24af5ffd-0eaa-49d5-9f88-984297d1dd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file Outputs/LMP_FTR_Mapping.csv already exists. Skipping processing.\n"
     ]
    }
   ],
   "source": [
    "file_path = 'Outputs/LMP_FTR_Mapping.csv'\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"The file {file_path} does not exist. Running the processing code...\")\n",
    "    matched_df.to_csv('Outputs/LMP_FTR_Mapping.csv')    # Make sure the directory exists before saving\n",
    "    print(f\"Processing complete. File saved to {file_path}\")\n",
    "else:\n",
    "    print(f\"The file {file_path} already exists. Skipping processing.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aed127-b830-4ee5-9431-a3cd810d29ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a8ad99-983a-4189-aa36-833cd8c97274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
